import logging
import os
import sys
import base64
import mimetypes
import uuid
import tempfile
import asyncio
from datetime import datetime
if os.getenv('API_ENV') != 'production':
    from dotenv import load_dotenv

    load_dotenv()


from fastapi import FastAPI, HTTPException, Request
from linebot.v3.webhook import WebhookParser
from linebot.v3.messaging import (
    AsyncApiClient,
    AsyncMessagingApi,
    Configuration,
    ReplyMessageRequest,
    PushMessageRequest,
    TextMessage,
    ImageMessage)
from linebot.v3.exceptions import (
    InvalidSignatureError
)
from linebot.v3.webhooks import (
    MessageEvent,
    TextMessageContent,
)
import google.generativeai as genai
from google import genai as genai_v2
from google.genai import types
from google.cloud import storage
import uvicorn
from firebase import firebase

logging.basicConfig(
    level=os.getenv('LOG', 'INFO'),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__file__)

app = FastAPI()

channel_secret = os.getenv('LINE_CHANNEL_SECRET', None)
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN', None)
if channel_secret is None:
    print('Specify LINE_CHANNEL_SECRET as environment variable.')
    sys.exit(1)
if channel_access_token is None:
    print('Specify LINE_CHANNEL_ACCESS_TOKEN as environment variable.')
    sys.exit(1)

configuration = Configuration(
    access_token=channel_access_token
)

parser = WebhookParser(channel_secret)


firebase_url = os.getenv('FIREBASE_URL')

# Gemini LLM è¨­å®šï¼ˆæ–‡å­—å°è©±ã€æ‘˜è¦ç­‰ï¼‰
gemini_llm_key = os.getenv('GEMINI_LLM_API_KEY')
gemini_llm_model = os.getenv('GEMINI_LLM_MODEL', 'gemini-1.5-pro')

# Gemini Image è¨­å®šï¼ˆåœ–ç‰‡ç”Ÿæˆï¼‰
gemini_image_key = os.getenv('GEMINI_IMAGE_API_KEY')
gemini_image_model = os.getenv('GEMINI_IMAGE_MODEL', 'gemini-2.5-flash-image-preview')

# ç‚ºäº†å‘å¾Œç›¸å®¹ï¼Œå¦‚æœæ²’æœ‰è¨­å®šåˆ†é›¢çš„ keyï¼Œå°±ä½¿ç”¨èˆŠçš„è¨­å®š
if not gemini_llm_key:
    gemini_llm_key = os.getenv('GEMINI_API_KEY')
if not gemini_image_key:
    gemini_image_key = os.getenv('GEMINI_API_KEY')
if not gemini_llm_model and os.getenv('GEMINI_MODEL'):
    gemini_llm_model = os.getenv('GEMINI_MODEL')

bot_line_id = os.getenv('LINE_BOT_ID', '377mwhqu')  # Bot çš„ LINE ID

# Google Cloud Storage è¨­å®š
gcs_bucket_name = os.getenv('GCS_BUCKET_NAME')  # ä½ çš„ Google Cloud Storage bucket åç¨±
gcs_credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')  # Google Cloud èªè­‰æª”æ¡ˆè·¯å¾‘


# Initialize the Gemini LLM API
genai.configure(api_key=gemini_llm_key)

# Initialize Google Cloud Storage client
if gcs_credentials_path and gcs_bucket_name:
    try:
        logging.info(f"Initializing Google Cloud Storage...")
        logging.info(f"GCS bucket name: {gcs_bucket_name}")
        logging.info(f"GCS credentials path: {gcs_credentials_path}")
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(gcs_bucket_name)
        
        # æ¸¬è©¦ bucket æ˜¯å¦å­˜åœ¨
        if bucket.exists():
            logging.info(f"Successfully connected to GCS bucket: {gcs_bucket_name}")
        else:
            logging.error(f"GCS bucket does not exist: {gcs_bucket_name}")
            bucket = None
            
    except Exception as e:
        logging.error(f"Failed to initialize Google Cloud Storage: {e}")
        storage_client = None
        bucket = None
else:
    logging.warning("Google Cloud Storage not configured. Image generation will be disabled.")
    logging.warning(f"GCS_BUCKET_NAME: {gcs_bucket_name}")
    logging.warning(f"GOOGLE_APPLICATION_CREDENTIALS: {gcs_credentials_path}")
    storage_client = None
    bucket = None


async def upload_image_to_gcs(image_data, filename):
    """
    ä¸Šå‚³åœ–ç‰‡åˆ° Google Cloud Storage ä¸¦è¿”å›å…¬é–‹ URL
    
    Args:
        image_data: åœ–ç‰‡çš„äºŒé€²ä½è³‡æ–™
        filename: æª”æ¡ˆåç¨±
    
    Returns:
        str: åœ–ç‰‡çš„å…¬é–‹ URLï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    logging.info(f"Starting upload_image_to_gcs - filename: {filename}")
    logging.info(f"Image data type: {type(image_data)}, size: {len(image_data) if image_data else 'None'}")
    
    if not bucket:
        logging.error("Google Cloud Storage not configured - bucket is None")
        logging.error(f"GCS bucket name: {gcs_bucket_name}")
        logging.error(f"GCS credentials path: {gcs_credentials_path}")
        return None
    
    try:
        # å»ºç«‹å”¯ä¸€çš„æª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_filename = f"linebot_images/{timestamp}_{filename}"
        logging.info(f"Generated unique filename: {unique_filename}")
        
        # ä¸Šå‚³åˆ° GCS
        logging.info(f"Creating blob in bucket: {bucket.name}")
        blob = bucket.blob(unique_filename)
        
        logging.info("Starting upload to GCS...")
        blob.upload_from_string(image_data)
        logging.info("Upload completed successfully")
        
        # å°æ–¼å•Ÿç”¨äº† uniform bucket-level access çš„ bucketï¼Œ
        # æˆ‘å€‘ä¸éœ€è¦å‘¼å« make_public()ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å…¬é–‹ URL
        logging.info("Generating public URL (uniform bucket-level access enabled)...")
        
        # ç›´æ¥æ§‹å»ºå…¬é–‹ URL
        public_url = f"https://storage.googleapis.com/{bucket.name}/{unique_filename}"
        logging.info(f"Image uploaded successfully: {public_url}")
        logging.info(f"Blob exists: {blob.exists()}")
        return public_url
        
    except Exception as e:
        logging.error(f"Failed to upload image to GCS: {e}")
        logging.error(f"Exception type: {type(e)}")
        import traceback
        logging.error(f"Traceback: {traceback.format_exc()}")
        return None


async def generate_image_with_gemini(prompt, max_retries=1, retry_delay=15):
    """
    ä½¿ç”¨ Gemini ç”Ÿæˆåœ–ç‰‡
    
    Args:
        prompt: åœ–ç‰‡ç”Ÿæˆçš„æç¤ºè©
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        retry_delay: é‡è©¦å»¶é²ï¼ˆç§’ï¼‰
    
    Returns:
        tuple: (æˆåŠŸç‹€æ…‹, çµæœè¨Šæ¯æˆ–åœ–ç‰‡URL)
    """
    logging.info(f"Starting generate_image_with_gemini with prompt: {prompt}")
    
    # æª¢æŸ¥åœ–ç‰‡ç”Ÿæˆ API è¨­å®š
    if not gemini_image_key:
        logging.error("Gemini Image API key not configured")
        return False, "åœ–ç‰‡ç”ŸæˆåŠŸèƒ½æœªè¨­å®š API Key"
    
    for attempt in range(max_retries + 1):
        if attempt > 0:
            logging.info(f"Retry attempt {attempt}/{max_retries} after {retry_delay} seconds...")
            await asyncio.sleep(retry_delay)
        
        try:
            logging.info(f"Creating Gemini Image client with API key: {gemini_image_key[:10]}...{gemini_image_key[-5:] if gemini_image_key else 'None'}")
            client = genai_v2.Client(api_key=gemini_image_key)
            
            # ä½¿ç”¨æ¸¬è©¦ä¸­æˆåŠŸçš„æ¨¡å‹
            model = "gemini-2.5-flash-image-preview"
            logging.info(f"Using image model: {model} (attempt {attempt + 1})")
            
            # ä½¿ç”¨ç°¡å–®ç›´æ¥çš„æç¤ºè©ï¼Œæ¸¬è©¦è­‰å¯¦æœ‰æ•ˆ
            prompts_to_try = [
                f"Create a photorealistic image of a {prompt}. Do not provide text description, only generate the actual image.",
                f"Generate image: {prompt}",
                f"Draw: {prompt}"
            ]
            
            current_prompt = prompts_to_try[min(attempt, len(prompts_to_try) - 1)]
            logging.info(f"Using prompt strategy {attempt + 1}: {current_prompt[:80]}...")
            
            # ä½¿ç”¨ç°¡å–®çš„å…§å®¹çµæ§‹ï¼Œèˆ‡æ¸¬è©¦ä¸­æˆåŠŸçš„ç›¸åŒ
            contents = [
                types.Content(
                    role="user",
                    parts=[
                        types.Part.from_text(text=current_prompt),
                    ],
                ),
            ]
            
            generate_content_config = types.GenerateContentConfig(
                response_modalities=["IMAGE", "TEXT"],
            )
            
            logging.info("Starting content generation stream...")
            
            # ç”Ÿæˆå…§å®¹
            image_url = None
            text_response = ""
            chunk_count = 0
            
            for chunk in client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generate_content_config,
            ):
                chunk_count += 1
                logging.info(f"Processing chunk {chunk_count}")
                
                # æª¢æŸ¥ chunk æ˜¯å¦æœ‰æœ‰æ•ˆçš„ candidates
                if (
                    not hasattr(chunk, 'candidates') or
                    chunk.candidates is None or
                    len(chunk.candidates) == 0 or
                    chunk.candidates[0].content is None or
                    chunk.candidates[0].content.parts is None or
                    len(chunk.candidates[0].content.parts) == 0
                ):
                    logging.warning(f"Chunk {chunk_count} has no valid content")
                    continue
                    
                part = chunk.candidates[0].content.parts[0]
                logging.info(f"Chunk {chunk_count} part type: {type(part)}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ inline_data
                if hasattr(part, 'inline_data') and part.inline_data:
                    logging.info(f"Found inline_data in chunk {chunk_count}: {type(part.inline_data)}")
                    if hasattr(part.inline_data, 'data') and part.inline_data.data:
                        logging.info(f"Found image data in chunk {chunk_count}")
                        inline_data = part.inline_data
                        image_data = inline_data.data
                        logging.info(f"Image data size: {len(image_data)} bytes")
                        logging.info(f"Image MIME type: {inline_data.mime_type}")
                        
                        file_extension = mimetypes.guess_extension(inline_data.mime_type) or '.png'
                        logging.info(f"File extension: {file_extension}")
                        
                        # å»ºç«‹æª”æ¡ˆåç¨±
                        safe_prompt = "".join(c for c in prompt if c.isalnum() or c in (' ', '-', '_')).rstrip()[:30]
                        filename = f"gemini_image_{safe_prompt}{file_extension}"
                        logging.info(f"Generated filename: {filename}")
                        
                        # ä¸Šå‚³åˆ° Google Cloud Storage
                        logging.info("Starting upload to GCS...")
                        image_url = await upload_image_to_gcs(image_data, filename)
                        logging.info(f"Upload result: {image_url}")
                        
                        # ä¸€æ—¦æ‰¾åˆ°åœ–ç‰‡å°±è·³å‡ºè¿´åœˆ
                        if image_url:
                            logging.info("Image found and uploaded successfully, breaking loop")
                            break
                    else:
                        logging.info(f"inline_data exists but no data: {part.inline_data}")
                else:
                    logging.info(f"No inline_data in chunk {chunk_count}")
                
                # è™•ç†æ–‡å­—å›æ‡‰
                if hasattr(part, 'text') and part.text:
                    text_response += part.text
                    logging.info(f"Received text in chunk {chunk_count}: {part.text[:100]}...")
                elif hasattr(chunk, 'text') and chunk.text:
                    text_response += chunk.text
                    logging.info(f"Received text from chunk object in chunk {chunk_count}: {chunk.text[:100]}...")
                else:
                    logging.info(f"Chunk {chunk_count} has no text data")
            
            logging.info(f"Finished processing {chunk_count} chunks")
            logging.info(f"Final image_url: {image_url}")
            logging.info(f"Final text_response: {text_response[:200]}...")
            
            if image_url:
                logging.info("Image generation successful")
                return True, image_url
            else:
                if text_response:
                    logging.warning(f"Model returned text only, no image generated. Text: {text_response[:200]}")
                    return False, f"âŒ æ¨¡å‹åªè¿”å›æ–‡å­—èªªæ˜è€Œæœªç”Ÿæˆåœ–ç‰‡ã€‚è«‹å˜—è©¦æ›´å…·é«”çš„æè¿°ï¼Œä¾‹å¦‚ï¼š'ä¸€ä½å°ç£å©¦å¥³åœ¨å‚³çµ±å¸‚å ´æŒ‘é¸æ–°é®®è”¬èœçš„çœŸå¯¦ç…§ç‰‡'"
                else:
                    return False, "âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                
        except Exception as e:
            logging.error(f"Error generating image with Gemini (attempt {attempt + 1}): {e}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé…é¡éŒ¯èª¤
            error_msg = str(e)
            is_quota_error = "429" in error_msg and "RESOURCE_EXHAUSTED" in error_msg
            is_rate_limit = "429" in error_msg
            
            if attempt < max_retries and is_rate_limit:
                logging.info(f"Rate limit hit, will retry in {retry_delay} seconds...")
                continue
            else:
                # æœ€å¾Œä¸€æ¬¡å˜—è©¦æˆ–éé‡è©¦éŒ¯èª¤
                if is_quota_error:
                    return False, "âŒ åœ–ç‰‡ç”Ÿæˆé…é¡å·²ç”¨ç›¡ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–å‡ç´šè‡³ä»˜è²»æ–¹æ¡ˆã€‚"
                elif "quota" in error_msg.lower():
                    return False, "âŒ API é…é¡ä¸è¶³ï¼Œè«‹æª¢æŸ¥æ‚¨çš„ Google AI ä½¿ç”¨é¡åº¦ã€‚"
                else:
                    return False, f"âŒ ç”Ÿæˆåœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    return False, "âŒ ç¶“éå¤šæ¬¡é‡è©¦ä»ç„¡æ³•ç”Ÿæˆåœ–ç‰‡ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"


def is_bot_mentioned(event, bot_id=None):
    """
    æª¢æŸ¥æ˜¯å¦ Bot è¢«æåŠ
    
    Args:
        event: LINE webhook event
        bot_id: Bot çš„ LINE IDï¼ˆå¯é¸ï¼‰
    
    Returns:
        bool: True å¦‚æœ Bot è¢«æåŠï¼ŒFalse å¦å‰‡
    """
    if not isinstance(event.message, TextMessageContent):
        return False
    
    text = event.message.text
    mention = getattr(event.message, 'mention', None)
    
    # æ–¹æ³•1: æª¢æŸ¥ mention ç‰©ä»¶ä¸­æ˜¯å¦åŒ…å«ç‰¹å®šçš„ç”¨æˆ¶ID
    if mention and hasattr(mention, 'mentionees'):
        # æ³¨æ„ï¼šé€™éœ€è¦çŸ¥é“ Bot çš„å¯¦éš› user_idï¼Œé€šå¸¸æ ¼å¼ç‚º Ué–‹é ­
        # ä½†æˆ‘å€‘å¯èƒ½ç„¡æ³•ç›´æ¥ç²å–åˆ° Bot è‡ªå·±çš„ user_id
        pass
    
    # æ–¹æ³•2: æª¢æŸ¥æ–‡å­—ä¸­æ˜¯å¦åŒ…å« Bot çš„å®˜æ–¹ ID
    if bot_id:
        # æª¢æŸ¥æ˜¯å¦åŒ…å« @bot_id æ ¼å¼ï¼ˆç¢ºä¿ @ å‰é¢æ²’æœ‰å…¶ä»–å­—ç¬¦ï¼‰
        import re
        bot_patterns = [
            rf'(?<![a-zA-Z0-9])@{re.escape(bot_id)}(?![a-zA-Z0-9])',
            rf'(?<![a-zA-Z0-9])ï¼ {re.escape(bot_id)}(?![a-zA-Z0-9])',
            rf'(?<![a-zA-Z0-9])@{re.escape(bot_id.lower())}(?![a-zA-Z0-9])',
            rf'(?<![a-zA-Z0-9])ï¼ {re.escape(bot_id.lower())}(?![a-zA-Z0-9])'
        ]
        
        for pattern in bot_patterns:
            if re.search(pattern, text):
                logging.info(f"Bot mentioned with pattern: {pattern}")
                return True
    
    # æ–¹æ³•3: æª¢æŸ¥æ˜¯å¦æœ‰ mention ä¸”æ–‡å­—åŒ…å«é—œéµè©
    if mention:
        # æª¢æŸ¥å¸¸è¦‹çš„ Bot å‘¼å«æ–¹å¼
        bot_keywords = ['bot', 'Bot', 'BOT', 'æ©Ÿå™¨äºº', 'æ‘˜è¦ç‹']
        if any(keyword in text for keyword in bot_keywords):
            logging.info(f"Bot mentioned with keyword in text: {text}")
            return True
    
    return False


@app.get("/health")
async def health():
    return 'ok'

@app.get("/")
async def root():
    return {"message": "LINE Bot is running", "status": "ok"}


@app.post("/webhooks/line")
async def handle_callback(request: Request):
    signature = request.headers['X-Line-Signature']

    # get request body as text
    body = await request.body()
    body = body.decode()

    try:
        events = parser.parse(body, signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")

    # å‰µå»º async client åœ¨ async å‡½æ•¸å…§
    async_api_client = AsyncApiClient(configuration)
    line_bot_api = AsyncMessagingApi(async_api_client)
    
    try:
        for event in events:
            logging.info(event)
            if not isinstance(event, MessageEvent):
                continue
            if not isinstance(event.message, TextMessageContent):
                continue
            text = event.message.text
            user_id = event.source.user_id

            msg_type = event.message.type
            fdb = firebase.FirebaseApplication(firebase_url, None)
            
            # è¨­å®š Firebase è·¯å¾‘
            if event.source.type == 'group':
                user_chat_path = f'groups/{event.source.group_id}'
            else:
                user_chat_path = f'users/{user_id}'
            
            # æ±ºå®šæ˜¯å¦è¦å›æ‡‰
            should_reply = False
            is_ai_question = False  # æ˜¯å¦ç‚º AI å•ç­”æ¨¡å¼
            special_commands = ['!æ¸…ç©º', '!clean',  '!æ‘˜è¦','!ç¸½çµ','!summary', 'ï¼æ¸…ç©º', 'ï¼æ‘˜è¦', '!help', '!å¹«åŠ©', 'ï¼help', 'ï¼å¹«åŠ©', '!ç•«åœ–', '!ç”Ÿæˆåœ–ç‰‡', 'ï¼ç•«åœ–', 'ï¼ç”Ÿæˆåœ–ç‰‡', '!image', '!draw']
            
            if event.source.type == 'group':
                # æª¢æŸ¥æ˜¯å¦çœŸçš„æåŠäº† Bot
                bot_mentioned = is_bot_mentioned(event, bot_line_id)
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®ŠæŒ‡ä»¤
                has_special_command = any(cmd in text.lower() for cmd in special_commands)
                
                if bot_mentioned and not has_special_command:
                    # Bot è¢«æåŠä½†ä¸æ˜¯ç‰¹æ®ŠæŒ‡ä»¤ = AI å•ç­”æ¨¡å¼
                    should_reply = True
                    is_ai_question = True
                    logging.info(f"Bot mentioned - AI question mode: '{text}'")
                elif has_special_command:
                    # ç‰¹æ®ŠæŒ‡ä»¤
                    should_reply = True
                    logging.info(f"Group message with special command: '{text}'")
                else:
                    logging.info(f"Recording group message (no reply): '{text}'")
            else:
                # ç§äººå°è©±ï¼šæ‰€æœ‰è¨Šæ¯éƒ½å›æ‡‰
                should_reply = True
                # æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ®ŠæŒ‡ä»¤
                has_special_command = any(cmd in text.lower() for cmd in special_commands)
                if not has_special_command:
                    # ä¸€èˆ¬å°è©±æ¨¡å¼
                    logging.info(f"Private conversation mode: '{text}'")
                else:
                    logging.info(f"Private message with special command: '{text}'")
            
            # ç²å–ç¾æœ‰å°è©±è¨˜éŒ„
            try:
                chatgpt = fdb.get(user_chat_path, 'messages')
                if chatgpt is None:
                    messages = []
                else:
                    messages = chatgpt if isinstance(chatgpt, list) else []
            except Exception as e:
                logging.warning(f"Failed to get messages from Firebase: {e}")
                messages = []

            if msg_type == 'text':
                # æ‰€æœ‰è¨Šæ¯éƒ½è¨˜éŒ„åˆ° Firebase
                messages.append({'role': 'user', 'parts': [text], 'timestamp': str(event.timestamp)})
                
                reply_msg = ""
                
                # åªæœ‰åœ¨éœ€è¦å›æ‡‰æ™‚æ‰è™•ç†
                if should_reply:
                    if text.lower() in ['!æ¸…ç©º', 'ï¼æ¸…ç©º', '!clean']:
                        try:
                            fdb.delete(user_chat_path, 'messages')
                            reply_msg = '------å°è©±æ­·å²ç´€éŒ„å·²ç¶“æ¸…ç©º------'
                            # æ¸…ç©ºå¾Œé‡ç½® messages
                            messages = []
                        except Exception as e:
                            logging.error(f"Failed to clear Firebase data: {e}")
                            reply_msg = 'æ¸…ç©ºå°è©±è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦'

                    elif text.lower() in ['!æ‘˜è¦', 'ï¼æ‘˜è¦', '!ç¸½çµ', 'ï¼ç¸½çµ', 'ï¼summary']:
                        if len(messages) > 1:  # ç¢ºä¿æœ‰å°è©±å…§å®¹å¯ä»¥æ‘˜è¦
                            try:
                                model = genai.GenerativeModel(gemini_llm_model)
                                # æº–å‚™çµ¦ Gemini çš„è¨Šæ¯æ ¼å¼ï¼ˆç§»é™¤ timestamp æ¬„ä½ï¼‰
                                gemini_messages = []
                                for msg in messages:
                                    gemini_msg = {
                                        'role': msg['role'],
                                        'parts': msg['parts']
                                    }
                                    gemini_messages.append(gemini_msg)
                                
                                response = model.generate_content(
                                    f'Summary the following message in Traditional Chinese by less 5 list points. \n{gemini_messages}')
                                reply_msg = response.text
                                # è¨˜éŒ„æ‘˜è¦å›æ‡‰
                                messages.append({'role': 'model', 'parts': [reply_msg], 'timestamp': str(event.timestamp)})
                            except Exception as e:
                                logging.error(f"Error generating summary: {e}")
                                reply_msg = "æŠ±æ­‰ï¼Œç”¢ç”Ÿæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                        else:
                            reply_msg = 'ç›®å‰æ²’æœ‰è¶³å¤ çš„å°è©±ç´€éŒ„å¯ä»¥æ‘˜è¦'
                            messages.append({'role': 'model', 'parts': [reply_msg], 'timestamp': str(event.timestamp)})
                    
                    elif text.lower() in ['!help', '!å¹«åŠ©', 'ï¼help', 'ï¼å¹«åŠ©']:
                        reply_msg = """ğŸ¤– ç¾¤çµ„æ‘˜è¦ç‹ ä½¿ç”¨èªªæ˜

**ç¾¤çµ„åŠŸèƒ½ï¼š**
â€¢ @ æ©Ÿå™¨äºº + å•é¡Œï¼šé€²å…¥ AI å•ç­”æ¨¡å¼
  ä¾‹ï¼š@Bot ä»€éº¼æ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿ

â€¢ !æ‘˜è¦ æˆ– ï¼æ‘˜è¦ï¼šç”¢ç”Ÿå°è©±æ‘˜è¦
â€¢ !æ¸…ç©º æˆ– ï¼æ¸…ç©ºï¼šæ¸…ç©ºå°è©±è¨˜éŒ„
â€¢ !ç•«åœ– [æè¿°] æˆ– ï¼ç•«åœ– [æè¿°]ï¼šç”Ÿæˆåœ–ç‰‡
  ä¾‹ï¼š!ç•«åœ– å¯æ„›çš„è²“å’ªåœ¨èŠ±åœ’è£¡ç©è€
  æç¤ºï¼šä½¿ç”¨å…·é«”ã€è©³ç´°çš„æè¿°æ•ˆæœæ›´å¥½
â€¢ !help æˆ– !å¹«åŠ©ï¼šé¡¯ç¤ºæ­¤èªªæ˜

**ç§äººåŠŸèƒ½ï¼š**
â€¢ ç›´æ¥å‚³é€è¨Šæ¯å³å¯èˆ‡ AI å°è©±
â€¢ æ”¯æ´æ‰€æœ‰ç¾¤çµ„æŒ‡ä»¤

**æ³¨æ„äº‹é …ï¼š**
â€¢ ç¾¤çµ„ä¸­åªæœ‰ @ æåŠæˆ–ç‰¹æ®ŠæŒ‡ä»¤æ‰æœƒå›æ‡‰
â€¢ AI å•ç­”ç‚ºä¸€æ¬¡æ€§å›ç­”ï¼Œä¸æœƒè¨˜éŒ„åˆ°å°è©±æ­·å²
â€¢ æ‰€æœ‰è¨Šæ¯éƒ½æœƒè¢«è¨˜éŒ„ä»¥ä¾›æ‘˜è¦åŠŸèƒ½ä½¿ç”¨
â€¢ åœ–ç‰‡ç”Ÿæˆéœ€è¦ Google Cloud Storage è¨­å®š"""
                        # å¹«åŠ©è¨Šæ¯ä¸è¨˜éŒ„åˆ°å°è©±æ­·å²
                        
                    elif any(cmd in text.lower() for cmd in ['!ç•«åœ–', 'ï¼ç•«åœ–', '!ç”Ÿæˆåœ–ç‰‡', 'ï¼ç”Ÿæˆåœ–ç‰‡', '!image', '!draw']):
                        # åœ–ç‰‡ç”ŸæˆåŠŸèƒ½
                        logging.info(f"Image generation command detected: {text}")
                        
                        if not bucket:
                            logging.error("Image generation requested but GCS not configured")
                            reply_msg = "æŠ±æ­‰ï¼Œåœ–ç‰‡ç”ŸæˆåŠŸèƒ½ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡è¨­å®š Google Cloud Storageã€‚"
                        else:
                            # æå–åœ–ç‰‡æè¿°
                            prompt = text
                            original_prompt = prompt
                            for cmd in ['!ç•«åœ–', 'ï¼ç•«åœ–', '!ç”Ÿæˆåœ–ç‰‡', 'ï¼ç”Ÿæˆåœ–ç‰‡', '!image', '!draw']:
                                if cmd in text.lower():
                                    prompt = text.lower().replace(cmd, '').strip()
                                    logging.info(f"Extracted prompt using command '{cmd}': '{prompt}'")
                                    break
                            
                            if not prompt:
                                logging.warning("No prompt provided for image generation")
                                reply_msg = "è«‹æä¾›åœ–ç‰‡æè¿°ï¼Œä¾‹å¦‚ï¼š!ç•«åœ– å¯æ„›çš„è²“å’ªåœ¨èŠ±åœ’è£¡ç©è€"
                            else:
                                logging.info(f"Starting image generation process with prompt: '{prompt}'")
                                
                                # å…ˆç™¼é€"ç”Ÿæˆä¸­"çš„è¨Šæ¯
                                await line_bot_api.reply_message(
                                    ReplyMessageRequest(
                                        reply_token=event.reply_token,
                                        messages=[TextMessage(text=f"ğŸ¨ æ­£åœ¨ç”Ÿæˆåœ–ç‰‡ï¼š{prompt}\nè«‹ç¨å€™...")]
                                    ))
                                logging.info("Sent 'generating' message to user")
                                
                                # ç”Ÿæˆåœ–ç‰‡
                                logging.info("Calling generate_image_with_gemini...")
                                success, result = await generate_image_with_gemini(prompt)
                                logging.info(f"Image generation result - success: {success}, result: {result}")
                                
                                if success:
                                    logging.info("Image generation successful, sending image message")
                                    # ç™¼é€åœ–ç‰‡è¨Šæ¯
                                    image_message = ImageMessage(
                                        original_content_url=result,
                                        preview_image_url=result
                                    )
                                    # ä½¿ç”¨ push message ç™¼é€åœ–ç‰‡ï¼ˆå› ç‚ºå·²ç¶“ç”¨äº† reply_tokenï¼‰
                                    if event.source.type == 'group':
                                        logging.info(f"Sending image to group: {event.source.group_id}")
                                        await line_bot_api.push_message(
                                            PushMessageRequest(
                                                to=event.source.group_id,
                                                messages=[image_message]
                                            )
                                        )
                                    else:
                                        logging.info(f"Sending image to user: {event.source.user_id}")
                                        await line_bot_api.push_message(
                                            PushMessageRequest(
                                                to=event.source.user_id,
                                                messages=[image_message]
                                            )
                                        )
                                    logging.info("Image message sent successfully")
                                    reply_msg = ""  # ä¸éœ€è¦é¡å¤–çš„æ–‡å­—å›æ‡‰
                                else:
                                    logging.error(f"Image generation failed: {result}")
                                    # ç™¼é€éŒ¯èª¤è¨Šæ¯
                                    error_message = TextMessage(text=f"âŒ {result}")
                                    if event.source.type == 'group':
                                        logging.info(f"Sending error message to group: {event.source.group_id}")
                                        await line_bot_api.push_message(
                                            PushMessageRequest(
                                                to=event.source.group_id,
                                                messages=[error_message]
                                            )
                                        )
                                    else:
                                        logging.info(f"Sending error message to user: {event.source.user_id}")
                                        await line_bot_api.push_message(
                                            PushMessageRequest(
                                                to=event.source.user_id,
                                                messages=[error_message]
                                            )
                                        )
                                    reply_msg = ""  # ä¸éœ€è¦é¡å¤–çš„æ–‡å­—å›æ‡‰
                        
                        # åœ–ç‰‡ç”ŸæˆæŒ‡ä»¤ä¸è¨˜éŒ„åˆ°å°è©±æ­·å²
                        messages.pop()  # ç§»é™¤å‰›æ‰åŠ å…¥çš„ç”¨æˆ¶è¨Šæ¯
                        logging.info("Removed image generation command from conversation history")
                        
                    elif is_ai_question:
                        # AI å•ç­”æ¨¡å¼ï¼šä¸€æ¬¡æ€§å›ç­”ï¼Œä¸è¨˜éŒ„åˆ°å°è©±æ­·å²ï¼ˆç¾¤çµ„ä¸­çš„ @ æåŠï¼‰
                        try:
                            model = genai.GenerativeModel(gemini_llm_model)
                            # ç§»é™¤ @ æåŠéƒ¨åˆ†ï¼Œåªä¿ç•™å•é¡Œ
                            clean_question = text
                            if hasattr(event.message, 'mention') and event.message.mention:
                                # å¦‚æœæœ‰ mention è³‡è¨Šï¼Œç§»é™¤è¢«æåŠçš„éƒ¨åˆ†
                                mention = event.message.mention
                                for mentioned_user in mention.mentionees:
                                    if mentioned_user.user_id:
                                        # ç°¡å–®çš„æ–‡å­—æ¸…ç†ï¼Œç§»é™¤å¯èƒ½çš„ @ ç¬¦è™Ÿ
                                        clean_question = text.replace('@', '').strip()
                            
                            response = model.generate_content(f"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œï¼š{clean_question}")
                            reply_msg = response.text
                            # AI å•ç­”ä¸è¨˜éŒ„åˆ°å°è©±æ­·å²ï¼Œæ‰€ä»¥ç§»é™¤å‰›åŠ å…¥çš„è¨Šæ¯
                            messages.pop()  # ç§»é™¤å‰›æ‰åŠ å…¥çš„ç”¨æˆ¶è¨Šæ¯
                        except Exception as e:
                            logging.error(f"Error in AI question mode: {e}")
                            reply_msg = "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                            messages.pop()  # ç§»é™¤å‰›æ‰åŠ å…¥çš„ç”¨æˆ¶è¨Šæ¯
                            
                    else:
                        # ä¸€èˆ¬å°è©±ï¼ˆç§äººå°è©±æˆ–ç¾¤çµ„ä¸­çš„å…¶ä»–æƒ…æ³ï¼‰
                        try:
                            model = genai.GenerativeModel(gemini_llm_model)
                            # æº–å‚™çµ¦ Gemini çš„è¨Šæ¯æ ¼å¼ï¼ˆç§»é™¤ timestamp æ¬„ä½ï¼‰
                            gemini_messages = []
                            for msg in messages:
                                gemini_msg = {
                                    'role': msg['role'],
                                    'parts': msg['parts']
                                }
                                gemini_messages.append(gemini_msg)
                            
                            response = model.generate_content(gemini_messages)
                            reply_msg = response.text
                            messages.append({'role': 'model', 'parts': [reply_msg], 'timestamp': str(event.timestamp)})
                            logging.info(f"Generated AI response for general conversation: {reply_msg[:50]}...")
                        except Exception as e:
                            logging.error(f"Error in general conversation: {e}")
                            reply_msg = "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                
                # æ›´æ–° Firebase ä¸­çš„å°è©±ç´€éŒ„
                # AI å•ç­”æ¨¡å¼ã€å¹«åŠ©è¨Šæ¯å’Œåœ–ç‰‡ç”ŸæˆæŒ‡ä»¤ä¸è¨˜éŒ„åˆ°å°è©±æ­·å²
                should_save_to_firebase = not is_ai_question and not (
                    text.lower() in ['!help', '!å¹«åŠ©', 'ï¼help', 'ï¼å¹«åŠ©'] or
                    any(cmd in text.lower() for cmd in ['!ç•«åœ–', 'ï¼ç•«åœ–', '!ç”Ÿæˆåœ–ç‰‡', 'ï¼ç”Ÿæˆåœ–ç‰‡', '!image', '!draw'])
                )
                
                if should_save_to_firebase:
                    try:
                        fdb.put(user_chat_path, 'messages', messages)
                        logging.info(f"Saved message to Firebase: {user_chat_path}")
                    except Exception as e:
                        logging.error(f"Failed to save to Firebase: {e}")
                else:
                    logging.info(f"Skipped saving to Firebase (special command): {text[:50]}...")

                # ç™¼é€å›æ‡‰ï¼ˆåªæœ‰åœ¨éœ€è¦å›æ‡‰ä¸”æœ‰è¨Šæ¯å…§å®¹æ™‚ï¼‰
                if should_reply and reply_msg:
                    await line_bot_api.reply_message(
                        ReplyMessageRequest(
                            reply_token=event.reply_token,
                            messages=[TextMessage(text=reply_msg)]
                        ))
    
    finally:
        # é—œé–‰ async client
        await async_api_client.close()

    return 'OK'

if __name__ == "__main__":
    port = int(os.environ.get('PORT', default=8080))
    debug = True if os.environ.get(
        'API_ENV', default='develop') == 'develop' else False
    logging.info('Application will start...')
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=debug)
